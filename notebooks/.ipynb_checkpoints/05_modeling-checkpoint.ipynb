{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cd25e6-8bf6-43eb-a7a3-0ab5f1eda382",
   "metadata": {},
   "source": [
    "## Objective of the Notebook\n",
    "(NoobAtem) Were going to create a experimental model before integrating into the model.py file. This would only take a sample of our dataset and our objective is to make sure we have the right configuration for each model that we will use.\n",
    "- YAMNnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddad60-01ff-4e10-9467-30d54d9cb10a",
   "metadata": {},
   "source": [
    "### Prerequesite Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c64d437-69d8-443b-a384-7bdd680e41ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 02:30:34.270145: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-29 02:30:34.272683: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-29 02:30:34.281452: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-29 02:30:34.296616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-29 02:30:34.300950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-29 02:30:34.311469: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-29 02:30:35.004736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import librosa\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import csv\n",
    "import scipy\n",
    "import joblib\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "matplotlib.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c467da-e7df-430e-a031-ecf2fb0a3ba3",
   "metadata": {},
   "source": [
    "### Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3469ad81-6176-4495-b360-444dba57a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main parent folder for handling in all sorts of data\n",
    "DATA_P: str = \"../data\"\n",
    "SRC_P: str = \"../src\"\n",
    "\n",
    "INTERIM_P: str = os.path.join(DATA_P, \"interim\") # Dataset that is being process and its under commission\n",
    "\n",
    "RAW_P: str = os.path.join(DATA_P, \"raw\") # Data that we've formatted and designed/collected\n",
    "AUDIO_INTERIM_P: str = os.path.join(INTERIM_P, \"audio\")\n",
    "IMAGE_INTERIM_P: str = os.path.join(INTERIM_P, \"image\")\n",
    "\n",
    "DOOR_RAW_P: str = os.path.join(RAW_P, \"door\")\n",
    "GLASS_RAW_P: str = os.path.join(RAW_P, \"glass\")\n",
    "FEATURE_INTERIM_P: str = os.path.join(INTERIM_P, \"feature.csv\")\n",
    "TARGET_INTERIM_P: str = os.path.join(INTERIM_P, \"target.csv\")\n",
    "\n",
    "WEIGHTS_P: str = os.path.join(SRC_P, \"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541a49c-b6d9-4928-adc4-3eb430dfc6a9",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336d28d-cf74-41a5-b1cf-ad36f54fe969",
   "metadata": {},
   "source": [
    "One interesting pre-trained model is the YAMNet. this model is used to classify sounds in a large quantity of labels. It has an inbuilt preprocess spectogram meaning it requires you to pass an audio than the spectogram. The code below is the mention step by step guide that tensorflow documentation layed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e06f0dd-6ef9-4494-978e-95a80df650df",
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b13bd2-13f9-4501-a0f5-84e6e910c4aa",
   "metadata": {},
   "source": [
    "Next, will extract the features using YAMNet from the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d1b12d-823c-4a2e-bbd9-7ef3a333aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files: list = os.listdir(AUDIO_INTERIM_P)\n",
    "features: list = []\n",
    "\n",
    "for p in audio_files:\n",
    "    audio_data, sr = librosa.load(os.path.join(AUDIO_INTERIM_P, p), sr=16000)\n",
    "    audio_data.astype(np.float32)\n",
    "    _, embeddings, _ = yamnet_model(audio_data)\n",
    "    features.append(embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99db4db5-af80-40f9-ad39-e9ccb3f7b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(yamnet_model: object, filepath: str) -> np.array:\n",
    "    audio_data, sr = librosa.load(filepath, sr=16000)\n",
    "    audio_data.astype(np.float32)\n",
    "    _, embeddings, _ = yamnet_model(audio_data)\n",
    "    return embeddings.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1eddf0-6e32-463b-923f-530855a5f065",
   "metadata": {},
   "source": [
    "Were now going take a sample of our dataset and split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c010d25-3b9e-4d37-8afe-32a3a5b8347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(doorpath: str, glasspath: str, yamnet_model: object) -> tuple:\n",
    "    X, y = [], []\n",
    "    for label in os.listdir(doorpath):\n",
    "        if label == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "        _label: str = label.split(\"-\")[0]\n",
    "        filepath: str = os.path.join(doorpath, label)\n",
    "        embeddings: np.array = extract_embeddings(yamnet_model, filepath)\n",
    "        X.append(embeddings)\n",
    "        y.append(_label)\n",
    "        \n",
    "    for label in os.listdir(glasspath):\n",
    "        if label == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "        _label: str = label.split(\"-\")[0]\n",
    "        filepath: str = os.path.join(glasspath, label)\n",
    "        embeddings: np.array = extract_embeddings(yamnet_model, filepath)\n",
    "        X.append(embeddings)\n",
    "        y.append(_label)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d706509-c57a-4b0c-b3d4-a09bf4d11bbb",
   "metadata": {},
   "source": [
    "Were are using the embeddings from the YAMNet then change the following label to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4c2789c-702d-4010-98e3-98645a2cf1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_prepare(DOOR_RAW_P, GLASS_RAW_P, yamnet_model)\n",
    "X: np.array = np.squeeze(X)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "332b54c1-c49f-4758-bde6-811ba0b22deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our labels\n",
    "class_names: list = list(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "576ed185-a09b-47ab-9839-8385d43ea26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491d600-cc0f-4a9d-8f98-8784d514c6cc",
   "metadata": {},
   "source": [
    "Lets defined model to fit our requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d626886-e5cd-491d-a4a4-b9bbe6c9e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf9325-174e-487b-9de5-fd7a5963274a",
   "metadata": {},
   "source": [
    "### Training with Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f733a9-d281-4b34-a0fb-69727cb68f93",
   "metadata": {},
   "source": [
    "With *YAMNet* pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6111ce7f-37cb-499a-974f-179055669573",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.fit(X_train, y_train, epochs=40, verbose=0, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be463b-d7d8-4b0c-8682-6cf44ac3c9db",
   "metadata": {},
   "source": [
    "### Evaluate and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f361e-4943-4595-ad36-9f2c1506a325",
   "metadata": {},
   "source": [
    "Lets check on the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74deb417-e676-4f77-8353-3b3fc78deb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0157\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1e99c-6656-46f9-b8c2-60e569e19021",
   "metadata": {},
   "source": [
    "Try to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac92bf4a-706c-44f3-b833-671885a429ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio_wav: str = os.path.join(GLASS_RAW_P, os.listdir(GLASS_RAW_P)[0])\n",
    "sample_label: str = sample_audio_wav.split(\"/\")[-1].split(\"-\")[0]\n",
    "sample_extract_emb: np.array = extract_embeddings(yamnet_model, sample_audio_wav)\n",
    "target_shape: tuple = (11, sample_extract_emb.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5034e01b-e8c0-4a5a-81f6-f558f373b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad or truncate embeddings to the required shape\n",
    "def pad_or_truncate(embeddings, target_shape):\n",
    "    current_shape = embeddings.shape\n",
    "    if current_shape[0] > target_shape[0]:\n",
    "        return embeddings[:target_shape[0], :]\n",
    "    elif current_shape[0] < target_shape[0]:\n",
    "        padding = np.zeros((target_shape[0] - current_shape[0], target_shape[1]))\n",
    "        return np.vstack((embeddings, padding))\n",
    "    else:\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c436da58-6d7f-4edc-afb9-4d4819a52055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'glass'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample_extract_emb = pad_or_truncate(sample_extract_emb, target_shape)\n",
    "sample_extract_emb = np.expand_dims(sample_extract_emb, axis=0)\n",
    "predictions: np.array = model.predict(sample_extract_emb)\n",
    "predicted_class: int = np.argmax(predictions, axis=-1)\n",
    "predicted_label: str = le.inverse_transform(predicted_class)[0]\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165fa75-ef3e-4ba4-8d20-5b21ad0bf5a3",
   "metadata": {},
   "source": [
    "For convenience, I'm converting this into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a06111-badc-474d-88d4-e80831b95b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8ad0228-29b4-4c63-b72f-4a0753f171ed",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871e55e-d240-4946-8afb-dde192fc0298",
   "metadata": {},
   "source": [
    "There is formats to naming the model that we should follow, this here would be the guide: mm-dd-yy-dataset_used-total_epochs.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be35a60b-98bd-42d6-a052-98621ce7b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for the the name dataset used, you will apply xor that will act as a decoder\n",
    "ESC50_DATA: int = 1\n",
    "RAW_DOOR_1_DATA: int = 2\n",
    "RAW_GLASS_2_DATA: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e87b6e3-7699-4ffc-9787-c36f33477208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../src/weights/07-29-2024-3-40.keras\n"
     ]
    }
   ],
   "source": [
    "now: str = datetime.now().strftime(\"%m-%d-%Y\")\n",
    "filename: str = now + \"-\" + str(ESC50_DATA | RAW_DOOR_1_DATA) + \"-40\"\n",
    "model.save(f'{os.path.join(WEIGHTS_P, filename)}.keras')\n",
    "print(f'{os.path.join(WEIGHTS_P, filename)}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b77cda2-9398-4a07-89e3-9b07ddd78941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../src/weights/label_encoder.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(le, os.path.join(\"../src\", \"weights\", 'label_encoder.joblib'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
